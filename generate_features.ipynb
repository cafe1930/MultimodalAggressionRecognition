{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchaudio\n",
    "import torchvision\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "import pickle\n",
    "\n",
    "#from trainer import TorchSupervisedTrainer\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from datasets import MultimodalPhysVerbDataset, AggrBatchSampler, AppendZeroValues, MultimodalDataset\n",
    "from trainer import MultimodalTrainer\n",
    "from models import PhysVerbClassifierConcatFeatures, AveragedFeaturesTransformerFusion, PhysVerbClassifierAddFeatures, PhysVerbClassifier, PhysVerbModel, TransformerSequenceProcessor, EqualSizedTransformerModalitiesFusion, MultimodalModel, CNN1D, Swin3d_T_extractor, OutputClassifier, MultiModalCrossEntropyLoss, Wav2vec2Extractor, Wav2vecExtractor, AudioCnn1DExtractorWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalFeatureGenDataset(MultimodalPhysVerbDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        output_data_tuple, output_labels_tuple = super().__getitem__(idx)\n",
    "        data_entry = self.time_intervals_df.loc[idx]\n",
    "        aggr_type = data_entry['aggr_type']\n",
    "        cluster_id = data_entry['cluster_id']\n",
    "        video_id = data_entry['video_id']\n",
    "        phys_t1 = data_entry['phys_t1']\n",
    "        phys_t2 = data_entry['phys_t2']\n",
    "        verb_t1 = data_entry['verb_t1']\n",
    "        verb_t2 = data_entry['verb_t2']\n",
    "        person_id = data_entry['person_id']\n",
    "        phys_label = data_entry['phys_aggr_label']\n",
    "        verb_label = data_entry['verb_aggr_label']\n",
    "        verb_name = f'c-{cluster_id}_{video_id}_{person_id}_{verb_t1/1000}-{verb_t2/1000}_{verb_label}'\n",
    "        phys_name = f'c-{cluster_id}_{video_id}_{person_id}_{phys_t1/1000}-{phys_t2/1000}_{phys_label}'\n",
    "        return output_data_tuple, output_labels_tuple, (verb_name, phys_name)\n",
    "    \n",
    "class PhysVerbModelFeat(PhysVerbModel):\n",
    "    def forward(self, input_data):\n",
    "        # извлечение признаков\n",
    "        modalities_features_dict = self.extract_features(input_data)\n",
    "        \n",
    "        # выполняем слияние модальностей\n",
    "        modalities_features_dict = self.modality_fusion_module(modalities_features_dict)\n",
    "        #return modalities_features_dict\n",
    "        # Выполнение классификации\n",
    "        '''\n",
    "        output_dict = {}\n",
    "        for aggr_type in self.classifiers:\n",
    "            output_dict[aggr_type] = self.classifiers[aggr_type](modalities_features_dict[aggr_type])\n",
    "        '''\n",
    "        output_dict = self.classifiers(modalities_features_dict)\n",
    "        return output_dict, modalities_features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mokhail\\AppData\\Local\\Temp\\ipykernel_19924\\4026364423.py:173: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(path_to_model)\n",
      "  0%|          | 0/4112 [00:00<?, ?it/s]c:\\Users\\mokhail\\python_programming\\MultimodalAggressionRecognition\\datasets.py:532: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  video = torch.load(path_to_video)\n",
      " 52%|█████▏    | 2127/4112 [15:54<12:41,  2.61it/s]c:\\Users\\mokhail\\python_programming\\MultimodalAggressionRecognition\\datasets.py:550: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  audio = torch.load(path_to_audio).to(self.device)\n",
      "c:\\Users\\mokhail\\python_programming\\MultimodalAggressionRecognition\\datasets.py:557: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  video = torch.load(path_to_video)\n",
      " 92%|█████████▏| 3783/4112 [27:40<02:49,  1.94it/s]c:\\Users\\mokhail\\python_programming\\MultimodalAggressionRecognition\\datasets.py:523: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  audio = torch.load(path_to_audio).to(self.device)\n",
      "100%|██████████| 4112/4112 [27:56<00:00,  2.45it/s]\n",
      "100%|██████████| 9516/9516 [1:10:30<00:00,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--path_to_dataset',  required=True)\n",
    "parser.add_argument('--path_to_intersections_csv')\n",
    "parser.add_argument('--path_to_train_test_split_json')\n",
    "parser.add_argument('--gpu_device_idx', type=int)\n",
    "parser.add_argument('--class_num', type=int)\n",
    "parser.add_argument('--resume_training', action='store_true')\n",
    "parser.add_argument('--path_to_checkpoint')\n",
    "parser.add_argument('--batch_size', required=True, type=int)\n",
    "parser.add_argument('--nn_input_size', nargs='+', type=int)\n",
    "parser.add_argument('--epoch_num', type=int)\n",
    "parser.add_argument('--test_size', type=float)\n",
    "parser.add_argument('--max_audio_len', type=int)\n",
    "parser.add_argument('--max_embeddings_len', type=int)\n",
    "parser.add_argument('--video_frames_num', type=int)\n",
    "parser.add_argument('--video_window_size', type=int)\n",
    "\n",
    "sample_args = [\n",
    "    '--path_to_dataset',\n",
    "    #r'/home/ubuntu/mikhail_u/DATASET_V0',\n",
    "    #r'/home/aggr/mikhail_u/DATA/DATSET_V0',\n",
    "    #r'C:\\Users\\admin\\python_programming\\DATA\\AVABOS\\DATSET_V0',\n",
    "    r'I:\\AVABOS\\DATSET_V0',\n",
    "    '--path_to_intersections_csv',\n",
    "    #r'/home/ubuntu/mikhail_u/DATASET_V0/time_intervals_combinations_table.csv',\n",
    "    #r'/home/aggr/mikhail_u/DATA/DATSET_V0/time_intervals_combinations_table.csv',\n",
    "    #r'C:\\Users\\admin\\python_programming\\DATA\\AVABOS\\DATSET_V0\\time_intervals_combinations_table.csv',\n",
    "    r'i:\\AVABOS\\DATSET_V0\\time_intervals_combinations_table.csv',\n",
    "    '--path_to_train_test_split_json',\n",
    "    r'train_test_split.json',\n",
    "    '--gpu_device_idx', '0',\n",
    "    '--class_num', '2',\n",
    "    '--epoch_num', '100',\n",
    "    '--batch_size', '1',\n",
    "    '--max_audio_len', '80000',\n",
    "    '--max_embeddings_len', '48',\n",
    "    '--video_frames_num', '128',\n",
    "    '--video_window_size', '8'\n",
    "    ]\n",
    "\n",
    "args = parser.parse_args(sample_args)\n",
    "\n",
    "path_to_dataset_root = args.path_to_dataset\n",
    "resume_training = args.resume_training\n",
    "path_to_intersections_csv = args.path_to_intersections_csv\n",
    "path_to_train_test_split_json = args.path_to_train_test_split_json\n",
    "\n",
    "path_to_checkpoint = args.path_to_checkpoint\n",
    "epoch_num = int(args.epoch_num)\n",
    "class_num = args.class_num\n",
    "batch_size = int(args.batch_size)\n",
    "max_audio_len = args.max_audio_len\n",
    "max_embeddings_len = args.max_embeddings_len\n",
    "video_frames_num = args.video_frames_num\n",
    "video_window_size = args.video_window_size\n",
    "gpu_device_idx = args.gpu_device_idx\n",
    "\n",
    "# имя модели соответствует имени экстрактора признаков\n",
    "phys_gamma_val = 2\n",
    "verb_gamma_val = 2\n",
    "model_name = 'A+T(ce)+fusion1L'\n",
    "modality2aggr = {'video':'phys', 'text':'verb', 'audio':'verb'}\n",
    "#modality2aggr = {'video':'verb', 'text':'verb', 'audio':'phys'}\n",
    "modalities_list = [\n",
    "    'audio',\n",
    "    'text',\n",
    "    'video'\n",
    "    ]\n",
    "aggr_types_list = set()\n",
    "for m in modalities_list:\n",
    "    aggr_types_list.add(modality2aggr[m])\n",
    "\n",
    "aggr_types_list = list(aggr_types_list)\n",
    "\n",
    "time_interval_combinations_df = pd.read_csv(path_to_intersections_csv)\n",
    "\n",
    "with open(path_to_train_test_split_json) as fd:\n",
    "    combinations_indices_dict = json.load(fd)\n",
    "\n",
    "train_time_interval_combinations_df =  []\n",
    "for cluster_id in combinations_indices_dict['train_clusters']:\n",
    "    df = time_interval_combinations_df[time_interval_combinations_df['cluster_id']==cluster_id]\n",
    "    train_time_interval_combinations_df.append(df)\n",
    "train_time_interval_combinations_df = pd.concat(train_time_interval_combinations_df, ignore_index=True)\n",
    "# для выравнивания баланса классов (баланс смещен в сторону не агрессивного поведения)\n",
    "# удалим не агрессивные интервалы физ. поведения, которые не пересекаются с вербальным поведением\n",
    "#drop_no_aggr_filter = (train_time_interval_combinations_df['aggr_type']=='phys')&(train_time_interval_combinations_df['phys_aggr_label']=='NOAGGR')\n",
    "#train_time_interval_combinations_df = train_time_interval_combinations_df[~drop_no_aggr_filter]\n",
    "# DEBUG\n",
    "#train_time_interval_combinations_df = train_time_interval_combinations_df.loc[0:500]\n",
    "\n",
    "test_time_interval_combinations_df =  []\n",
    "for cluster_id in combinations_indices_dict['test_clusters']:\n",
    "    df = time_interval_combinations_df[time_interval_combinations_df['cluster_id']==cluster_id]\n",
    "    test_time_interval_combinations_df.append(df)\n",
    "test_time_interval_combinations_df = pd.concat(test_time_interval_combinations_df, ignore_index=True)\n",
    "\n",
    "device = torch.device(f'cuda:{gpu_device_idx}')\n",
    "\n",
    "train_video_transform = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomAffine(degrees=20, translate=(0.1, 0.3), scale=(0.6, 1.1), shear=10),\n",
    "    v2.RandomPerspective(distortion_scale=0.2),\n",
    "    v2.Resize((112, 112), antialias=True),\n",
    "    AppendZeroValues(target_size=[video_frames_num, 3, 112, 112]),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_video_transform = v2.Compose([\n",
    "    v2.Resize((112, 112), antialias=True),\n",
    "    AppendZeroValues(target_size=[video_frames_num, 3, 112, 112]),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_audio_transform = v2.Compose([AppendZeroValues(target_size=[max_audio_len])])\n",
    "test_audio_transform = v2.Compose([AppendZeroValues(target_size=[max_audio_len])])\n",
    "\n",
    "train_text_transform = v2.Compose([AppendZeroValues(target_size=[max_embeddings_len, 768])])\n",
    "test_text_transform = v2.Compose([AppendZeroValues(target_size=[max_embeddings_len, 768])])\n",
    "\n",
    "train_transforms_dict = {\n",
    "    'audio': train_audio_transform,\n",
    "    'text': train_text_transform,\n",
    "    'video': train_video_transform\n",
    "}\n",
    "train_transforms_dict = {k:v for k,v in train_transforms_dict.items()if k in modalities_list}\n",
    "test_transforms_dict = {\n",
    "    'audio': test_audio_transform,\n",
    "    'text': test_text_transform,\n",
    "    'video': test_video_transform\n",
    "}\n",
    "test_transforms_dict = {k:v for k,v in test_transforms_dict.items()if k in modalities_list}\n",
    "\n",
    "\n",
    "train_dataset = MultimodalFeatureGenDataset(\n",
    "    time_intervals_df=train_time_interval_combinations_df,\n",
    "    path_to_dataset=path_to_dataset_root,\n",
    "    modality_augmentation_dict=train_transforms_dict,\n",
    "    modality2aggr=modality2aggr,\n",
    "    actual_modalities_list=modalities_list,\n",
    "    device=device,\n",
    "    text_embedding_type='ru_conversational_cased_L-12_H-768_A-12_pt_v1_tokens'\n",
    "    )\n",
    "test_dataset = MultimodalFeatureGenDataset(\n",
    "    time_intervals_df=test_time_interval_combinations_df,\n",
    "    path_to_dataset=path_to_dataset_root,\n",
    "    modality_augmentation_dict=train_transforms_dict,\n",
    "    modality2aggr=modality2aggr,\n",
    "    actual_modalities_list=modalities_list,\n",
    "    device=device,\n",
    "    text_embedding_type='ru_conversational_cased_L-12_H-768_A-12_pt_v1_tokens'\n",
    "    )\n",
    "\n",
    "train_batch_sampler = AggrBatchSampler(train_time_interval_combinations_df, batch_size=batch_size, shuffle=True)\n",
    "test_batch_sampler = AggrBatchSampler(test_time_interval_combinations_df, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=train_batch_sampler,\n",
    "    num_workers=0\n",
    "    #pin_memory=True\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_sampler=test_batch_sampler,\n",
    "    num_workers=0\n",
    "    #pin_memory=True\n",
    ")\n",
    "\n",
    "path_to_model = r'I:\\AVABOS\\saving_dir2\\saving_dir\\12.11.2024, 15-19-48 (V(focal,g=1)+A+T(ce)+fusion1L)\\verb_best_ep-3.pt'\n",
    "model = torch.load(path_to_model)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "class PidorEbaniy(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, input_data):\n",
    "        # извлечение признаков\n",
    "        modalities_features_dict = self.model.extract_features(input_data)\n",
    "        \n",
    "        # выполняем слияние модальностей\n",
    "        modalities_features_dict = self.model.modality_fusion_module(modalities_features_dict)\n",
    "        #return modalities_features_dict\n",
    "        # Выполнение классификации\n",
    "        '''\n",
    "        output_dict = {}\n",
    "        for aggr_type in self.classifiers:\n",
    "            output_dict[aggr_type] = self.classifiers[aggr_type](modalities_features_dict[aggr_type])\n",
    "        '''\n",
    "        output_dict = self.model.classifiers(modalities_features_dict)\n",
    "        concat_features = []\n",
    "        for modality in ['audio', 'text']:\n",
    "            features = modalities_features_dict[modality]\n",
    "            features = features.mean(dim=1)\n",
    "            concat_features.append(features)\n",
    "        concat_features = torch.cat(concat_features, dim=1)\n",
    "        return output_dict, concat_features\n",
    "\n",
    "pedota_ebanaya = PidorEbaniy(model)\n",
    "#with torch.no_grad():\n",
    "files_lst = []\n",
    "preds_list = []\n",
    "true_list = []\n",
    "for data, labels, (verb_name, phys_name) in tqdm(test_dataloader):\n",
    "\n",
    "    ret_dict, multimodal_features = pedota_ebanaya(data)\n",
    "    \n",
    "    verb_name = verb_name[0]\n",
    "    filtered_labels = [v for v in labels if v[0][0]=='verb']\n",
    "    if len(filtered_labels) > 0:\n",
    "        aggr_type, label = filtered_labels[0]\n",
    "        label = label.item()\n",
    "        true_list.append(label)\n",
    "        one_hot_label = np.zeros((2,), dtype=int)\n",
    "        one_hot_label[label] = 1\n",
    "        res = ret_dict['verb']\n",
    "        res = res.detach()\n",
    "        _, pred_label = res.max(dim=1)\n",
    "        pred_label = pred_label.item()\n",
    "        preds_list.append(pred_label)\n",
    "        one_hot_pred_label = np.zeros((2,), dtype=int)\n",
    "        one_hot_pred_label[pred_label] = 1\n",
    "        multimodal_features = multimodal_features.squeeze(0).detach().cpu().numpy()\n",
    "        d = {'filename':verb_name,'features':multimodal_features, 'targets':one_hot_label, 'predictions':one_hot_pred_label}\n",
    "        files_lst.append(d)\n",
    "with open('test_set.pkl', 'wb') as fd:\n",
    "    pickle.dump(files_lst, fd)\n",
    "        \n",
    "for data, labels, (verb_name, phys_name) in tqdm(train_dataloader):\n",
    "\n",
    "    ret_dict, multimodal_features = pedota_ebanaya(data)\n",
    "    \n",
    "    verb_name = verb_name[0]\n",
    "    filtered_labels = [v for v in labels if v[0][0]=='verb']\n",
    "    if len(filtered_labels) > 0:\n",
    "        aggr_type, label = filtered_labels[0]\n",
    "        label = label.item()\n",
    "        true_list.append(label)\n",
    "        one_hot_label = np.zeros((2,), dtype=int)\n",
    "        one_hot_label[label] = 1\n",
    "        res = ret_dict['verb']\n",
    "        res = res.detach()\n",
    "        _, pred_label = res.max(dim=1)\n",
    "        pred_label = pred_label.item()\n",
    "        preds_list.append(pred_label)\n",
    "        one_hot_pred_label = np.zeros((2,), dtype=int)\n",
    "        one_hot_pred_label[pred_label] = 1\n",
    "        multimodal_features = multimodal_features.squeeze(0).detach().cpu().numpy()\n",
    "        d = {'filename':verb_name,'features':multimodal_features, 'targets':one_hot_label, 'predictions':one_hot_pred_label}\n",
    "        files_lst.append(d)\n",
    "        \n",
    "\n",
    "with open('train_set.pkl', 'wb') as fd:\n",
    "    pickle.dump(files_lst, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.706     0.806     0.753       732\n",
      "           1      0.876     0.804     0.838      1253\n",
      "\n",
      "    accuracy                          0.805      1985\n",
      "   macro avg      0.791     0.805     0.796      1985\n",
      "weighted avg      0.813     0.805     0.807      1985\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(true_list, preds_list, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1536])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for d in test_dataloader:\n",
    "    break\n",
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--path_to_dataset',  required=True)\n",
    "parser.add_argument('--path_to_intersections_csv')\n",
    "parser.add_argument('--path_to_train_test_split_json')\n",
    "parser.add_argument('--gpu_device_idx', type=int)\n",
    "parser.add_argument('--class_num', type=int)\n",
    "parser.add_argument('--resume_training', action='store_true')\n",
    "parser.add_argument('--path_to_checkpoint')\n",
    "parser.add_argument('--batch_size', required=True, type=int)\n",
    "parser.add_argument('--nn_input_size', nargs='+', type=int)\n",
    "parser.add_argument('--epoch_num', type=int)\n",
    "parser.add_argument('--test_size', type=float)\n",
    "parser.add_argument('--max_audio_len', type=int)\n",
    "parser.add_argument('--max_embeddings_len', type=int)\n",
    "parser.add_argument('--video_frames_num', type=int)\n",
    "parser.add_argument('--video_window_size', type=int)\n",
    "\n",
    "sample_args = [\n",
    "    '--path_to_dataset',\n",
    "    #r'/home/ubuntu/mikhail_u/DATASET_V0',\n",
    "    #r'/home/aggr/mikhail_u/DATA/DATSET_V0',\n",
    "    #r'C:\\Users\\admin\\python_programming\\DATA\\AVABOS\\DATSET_V0',\n",
    "    r'I:\\AVABOS\\DATSET_V0',\n",
    "    '--path_to_intersections_csv',\n",
    "    #r'/home/ubuntu/mikhail_u/DATASET_V0/time_intervals_combinations_table.csv',\n",
    "    #r'/home/aggr/mikhail_u/DATA/DATSET_V0/time_intervals_combinations_table.csv',\n",
    "    #r'C:\\Users\\admin\\python_programming\\DATA\\AVABOS\\DATSET_V0\\time_intervals_combinations_table.csv',\n",
    "    r'i:\\AVABOS\\DATSET_V0\\time_intervals_combinations_table.csv',\n",
    "    '--path_to_train_test_split_json',\n",
    "    r'train_test_split.json',\n",
    "    '--gpu_device_idx', '0',\n",
    "    '--class_num', '2',\n",
    "    '--epoch_num', '100',\n",
    "    '--batch_size', '1',\n",
    "    '--max_audio_len', '80000',\n",
    "    '--max_embeddings_len', '48',\n",
    "    '--video_frames_num', '128',\n",
    "    '--video_window_size', '8'\n",
    "    ]\n",
    "\n",
    "args = parser.parse_args(sample_args)\n",
    "\n",
    "path_to_dataset_root = args.path_to_dataset\n",
    "resume_training = args.resume_training\n",
    "path_to_intersections_csv = args.path_to_intersections_csv\n",
    "path_to_train_test_split_json = args.path_to_train_test_split_json\n",
    "\n",
    "path_to_checkpoint = args.path_to_checkpoint\n",
    "epoch_num = int(args.epoch_num)\n",
    "class_num = args.class_num\n",
    "batch_size = int(args.batch_size)\n",
    "max_audio_len = args.max_audio_len\n",
    "max_embeddings_len = args.max_embeddings_len\n",
    "video_frames_num = args.video_frames_num\n",
    "video_window_size = args.video_window_size\n",
    "gpu_device_idx = args.gpu_device_idx\n",
    "\n",
    "# имя модели соответствует имени экстрактора признаков\n",
    "phys_gamma_val = 2\n",
    "verb_gamma_val = 2\n",
    "model_name = 'A+T(ce)+fusion1L'\n",
    "modality2aggr = {'video':'phys', 'text':'verb', 'audio':'verb'}\n",
    "#modality2aggr = {'video':'verb', 'text':'verb', 'audio':'phys'}\n",
    "modalities_list = [\n",
    "    'audio',\n",
    "    'text',\n",
    "    'video'\n",
    "    ]\n",
    "aggr_types_list = set()\n",
    "for m in modalities_list:\n",
    "    aggr_types_list.add(modality2aggr[m])\n",
    "\n",
    "aggr_types_list = list(aggr_types_list)\n",
    "\n",
    "time_interval_combinations_df = pd.read_csv(path_to_intersections_csv)\n",
    "\n",
    "with open(path_to_train_test_split_json) as fd:\n",
    "    combinations_indices_dict = json.load(fd)\n",
    "\n",
    "train_time_interval_combinations_df =  []\n",
    "for cluster_id in combinations_indices_dict['train_clusters']:\n",
    "    df = time_interval_combinations_df[time_interval_combinations_df['cluster_id']==cluster_id]\n",
    "    train_time_interval_combinations_df.append(df)\n",
    "train_time_interval_combinations_df = pd.concat(train_time_interval_combinations_df, ignore_index=True)\n",
    "# для выравнивания баланса классов (баланс смещен в сторону не агрессивного поведения)\n",
    "# удалим не агрессивные интервалы физ. поведения, которые не пересекаются с вербальным поведением\n",
    "#drop_no_aggr_filter = (train_time_interval_combinations_df['aggr_type']=='phys')&(train_time_interval_combinations_df['phys_aggr_label']=='NOAGGR')\n",
    "#train_time_interval_combinations_df = train_time_interval_combinations_df[~drop_no_aggr_filter]\n",
    "# DEBUG\n",
    "#train_time_interval_combinations_df = train_time_interval_combinations_df.loc[0:500]\n",
    "\n",
    "test_time_interval_combinations_df =  []\n",
    "for cluster_id in combinations_indices_dict['test_clusters']:\n",
    "    df = time_interval_combinations_df[time_interval_combinations_df['cluster_id']==cluster_id]\n",
    "    test_time_interval_combinations_df.append(df)\n",
    "test_time_interval_combinations_df = pd.concat(test_time_interval_combinations_df, ignore_index=True)\n",
    "\n",
    "\n",
    "device = torch.device(f'cuda:{gpu_device_idx}')\n",
    "\n",
    "train_video_transform = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomAffine(degrees=20, translate=(0.1, 0.3), scale=(0.6, 1.1), shear=10),\n",
    "    v2.RandomPerspective(distortion_scale=0.2),\n",
    "    v2.Resize((112, 112), antialias=True),\n",
    "    AppendZeroValues(target_size=[video_frames_num, 3, 112, 112]),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_video_transform = v2.Compose([\n",
    "    v2.Resize((112, 112), antialias=True),\n",
    "    AppendZeroValues(target_size=[video_frames_num, 3, 112, 112]),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_audio_transform = v2.Compose([AppendZeroValues(target_size=[max_audio_len])])\n",
    "test_audio_transform = v2.Compose([AppendZeroValues(target_size=[max_audio_len])])\n",
    "\n",
    "\n",
    "\n",
    "train_text_transform = v2.Compose([AppendZeroValues(target_size=[max_embeddings_len, 768])])\n",
    "test_text_transform = v2.Compose([AppendZeroValues(target_size=[max_embeddings_len, 768])])\n",
    "\n",
    "train_transforms_dict = {\n",
    "    'audio': train_audio_transform,\n",
    "    'text': train_text_transform,\n",
    "    'video': train_video_transform\n",
    "}\n",
    "train_transforms_dict = {k:v for k,v in train_transforms_dict.items()if k in modalities_list}\n",
    "test_transforms_dict = {\n",
    "    'audio': test_audio_transform,\n",
    "    'text': test_text_transform,\n",
    "    'video': test_video_transform\n",
    "}\n",
    "test_transforms_dict = {k:v for k,v in test_transforms_dict.items()if k in modalities_list}\n",
    "\n",
    "\n",
    "train_dataset = MultimodalFeatureGenDataset(\n",
    "    time_intervals_df=train_time_interval_combinations_df,\n",
    "    path_to_dataset=path_to_dataset_root,\n",
    "    modality_augmentation_dict=train_transforms_dict,\n",
    "    modality2aggr=modality2aggr,\n",
    "    actual_modalities_list=modalities_list,\n",
    "    device=device,\n",
    "    text_embedding_type='ru_conversational_cased_L-12_H-768_A-12_pt_v1_tokens'\n",
    "    )\n",
    "test_dataset = MultimodalFeatureGenDataset(\n",
    "    time_intervals_df=test_time_interval_combinations_df,\n",
    "    path_to_dataset=path_to_dataset_root,\n",
    "    modality_augmentation_dict=train_transforms_dict,\n",
    "    modality2aggr=modality2aggr,\n",
    "    actual_modalities_list=modalities_list,\n",
    "    device=device,\n",
    "    text_embedding_type='ru_conversational_cased_L-12_H-768_A-12_pt_v1_tokens'\n",
    "    )\n",
    "\n",
    "\n",
    "train_batch_sampler = AggrBatchSampler(train_time_interval_combinations_df, batch_size=batch_size, shuffle=True)\n",
    "test_batch_sampler = AggrBatchSampler(test_time_interval_combinations_df, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=train_batch_sampler,\n",
    "    num_workers=0\n",
    "    #pin_memory=True\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_sampler=test_batch_sampler,\n",
    "    num_workers=0\n",
    "    #pin_memory=True\n",
    ")\n",
    "\n",
    "# вычисляем веса классов для физичекой и вербальной агрессии\n",
    "phys_aggr_filter = (train_time_interval_combinations_df['aggr_type'] == 'phys')\n",
    "verb_aggr_filter = (train_time_interval_combinations_df['aggr_type'] == 'verb')\n",
    "phys_verb_agr_filter = (train_time_interval_combinations_df['aggr_type'] == 'phys&verb')\n",
    "phys_aggr_df = train_time_interval_combinations_df[phys_aggr_filter]\n",
    "verb_aggr_df = train_time_interval_combinations_df[verb_aggr_filter]\n",
    "phys_verb_aggr_df = train_time_interval_combinations_df[phys_verb_agr_filter]\n",
    "all_phys_aggr_df = train_time_interval_combinations_df[phys_aggr_filter|phys_verb_agr_filter]\n",
    "all_verb_aggr_df = train_time_interval_combinations_df[verb_aggr_filter|phys_verb_agr_filter]\n",
    "\n",
    "verb_weights_series = 1-all_verb_aggr_df['verb_aggr_label'].value_counts()/len(all_verb_aggr_df)\n",
    "phys_weights_series = 1-all_phys_aggr_df['phys_aggr_label'].value_counts()/len(all_phys_aggr_df)\n",
    "\n",
    "verb_weights = torch.zeros([class_num], device=device)\n",
    "phys_weights = torch.zeros([class_num], device=device)\n",
    "\n",
    "verb_weights[0] = verb_weights_series['NOAGGR']\n",
    "verb_weights[1] = verb_weights_series['AGGR']\n",
    "\n",
    "phys_weights[0] = phys_weights_series['NOAGGR']\n",
    "phys_weights[1] = phys_weights_series['AGGR']\n",
    "\n",
    "phys_focal_loss = torch.hub.load(\n",
    "        'adeelh/pytorch-multi-class-focal-loss',\n",
    "        model='FocalLoss',\n",
    "        alpha=phys_weights,\n",
    "        gamma=phys_gamma_val,\n",
    "        reduction='mean',\n",
    "        force_reload=False\n",
    "    )\n",
    "\n",
    "audio_extractor = AudioCnn1DExtractorWrapper(hidden_size=768)\n",
    "\n",
    "\n",
    "audio_model = TransformerSequenceProcessor(\n",
    "    extractor_model=audio_extractor,\n",
    "    transformer_layer_num=1,\n",
    "    transformer_head_num=8,\n",
    "    hidden_size=768,\n",
    "    class_num=class_num\n",
    "    )\n",
    "text_model = TransformerSequenceProcessor(\n",
    "    extractor_model=nn.Sequential(),\n",
    "    transformer_layer_num=1,\n",
    "    transformer_head_num=8,\n",
    "    hidden_size=768,\n",
    "    class_num=2\n",
    ")\n",
    "\n",
    "# dummy class\n",
    "class E(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.e = Swin3d_T_extractor(frame_num=video_frames_num, window_size=video_window_size)\n",
    "    def forward(self, x, ret_type='PIDOR EPTA'):\n",
    "        return self.e(x)\n",
    "\n",
    "video_extractor = Swin3d_T_extractor(frame_num=video_frames_num, window_size=video_window_size)\n",
    "\n",
    "video_model = TransformerSequenceProcessor(\n",
    "    extractor_model=video_extractor,\n",
    "    transformer_layer_num=1,\n",
    "    transformer_head_num=8,\n",
    "    hidden_size=768,\n",
    "    class_num=class_num\n",
    "    )\n",
    "    \n",
    "# определяем размерности векторов признаков для многомодальной обработки\n",
    "video_features_shape = video_model(torch.zeros([1, 3, video_frames_num, 112, 112])).shape\n",
    "audio_features_shape = audio_extractor(torch.zeros([1, max_audio_len])).shape\n",
    "text_features_shape = text_model(torch.zeros([1, max_embeddings_len, 768])).shape\n",
    "modality_features_shapes_dict = {\n",
    "    'audio':list(audio_features_shape)[1:],\n",
    "    'text':list(text_features_shape)[1:],\n",
    "    'video':list(video_features_shape)[1:]\n",
    "}\n",
    "\n",
    "'''\n",
    "print(f'video_Shape={torch.zeros([1, 3, video_frames_num, 112, 112]).shape}')\n",
    "print(f'audio_Shape={torch.zeros([1, max_audio_len]).shape}')\n",
    "print(f'text_Shape= {torch.zeros([1, max_embeddings_len, 768]).shape}')\n",
    "print(modality_features_shapes_dict)\n",
    "exit()\n",
    "'''\n",
    "modality_features_shapes_dict = {k:v for k,v in modality_features_shapes_dict.items() if k in modalities_list}\n",
    "modality_extractors_dict = {\n",
    "    'audio':audio_extractor,\n",
    "    'text':nn.Sequential(),\n",
    "    #'text':text_model,\n",
    "    #'text':text_model,\n",
    "    'video':video_extractor\n",
    "    #'video':video_model\n",
    "}\n",
    "modality_extractors_dict = {k:v for k,v in modality_extractors_dict.items() if k in modalities_list}\n",
    "modality_extractors_dict = nn.ModuleDict(modality_extractors_dict)\n",
    "\n",
    "modality_fusion_module = EqualSizedTransformerModalitiesFusion(fusion_transformer_layer_num=1, fusion_transformer_hidden_size=768, fusion_transformer_head_num=8)\n",
    "\n",
    "modalities_adaptors_inout_sizes_dict = {'video':[video_features_shape[-1], 768], 'audio':[audio_features_shape[-1], 768], 'text':[text_features_shape[-1], 768]}\n",
    "aggr_classifiers = PhysVerbClassifierConcatFeatures(\n",
    "    modalities_list=modalities_list,\n",
    "    class_num=2,\n",
    "    modalities_adaptors_inout_sizes_dict=modalities_adaptors_inout_sizes_dict,\n",
    "    modality2aggr=modality2aggr\n",
    "    )\n",
    "model = PhysVerbModel(\n",
    "    modality_extractors_dict=modality_extractors_dict,\n",
    "    modality_fusion_module=modality_fusion_module,\n",
    "    #modality_fusion_module=nn.Sequential(),\n",
    "    classifiers=aggr_classifiers,\n",
    "    modality_features_shapes_dict=modality_features_shapes_dict,\n",
    "    modality2aggr=modality2aggr,\n",
    "    hidden_size=768,\n",
    "    class_num=2)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for data, labels, (verb_name, phys_name) in tqdm(test_dataloader):\n",
    "\n",
    "    ret_dict = model(data)\n",
    "    \n",
    "    verb_name = verb_name[0]\n",
    "    filtered_labels = [v for v in labels if v[0]=='verb']\n",
    "    if len(filtered_labels) > 0:\n",
    "        aggr_type, label = filtered_labels[0]\n",
    "        label = label.item()\n",
    "        one_hot_label = np.zeros((2,), dtype=int)\n",
    "        one_hot_label[label] = 1\n",
    "        res = ret['verb']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phys': tensor([[0.0926, 0.0431]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
       " 'verb': tensor([[ 0.0345, -0.1720]], device='cuda:0', grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggr_type, label = labels[0]\n",
    "label = label.item()\n",
    "one_hot_label = np.zeros((2,), dtype=int)\n",
    "one_hot_label[label] = 1\n",
    "one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('c-2_v-2-0_F-7-0_-0.001--0.001_-1',)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('verb_EMPTY',), tensor([-1], device='cuda:0')],\n",
       " [('phys',), tensor([0], device='cuda:0')]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phys': tensor([[-0.0234,  0.0085]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
       " 'verb': tensor([[0.0880, 0.0727]], device='cuda:0', grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_checkpoint = r'saving_dir\\12.11.2024, 00-37-51 (A+T(ce)+fus1L)\\A+T(ce)+fus1L_current_ep-51.pt'\n",
    "trainer = torch.load(path_to_checkpoint)\n",
    "saved_model = trainer.model\n",
    "#state_dict = saved_model.state_dict()\n",
    "#model.load_state_dict(state_dict=state_dict)\n",
    "saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY',\n",
       "   'audio_EMPTY'),\n",
       "  tensor([[-1.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [-1.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [-1.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [-1.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [-1.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [-1.,  0.,  0.,  ...,  0.,  0.,  0.]], device='cuda:0')],\n",
       " [('text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY',\n",
       "   'text_EMPTY'),\n",
       "  tensor([[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           ...,\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "  \n",
       "          [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           ...,\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "  \n",
       "          [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           ...,\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           ...,\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "  \n",
       "          [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           ...,\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "  \n",
       "          [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           ...,\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "           [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]], device='cuda:0')]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
